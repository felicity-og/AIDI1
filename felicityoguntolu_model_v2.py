# -*- coding: utf-8 -*-
"""FelicityOguntolu_model_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fvm-c9m3GvxN5MWmIP5WnxTqs40k0Wqr

#Breast Cancer Diagnosis
###Classification problem (UCI dataset)

###Step 1: Import Libraries and Load Data

We start by importing the necessary libraries and loading the Heart Disease UCI dataset.
"""

!pip install ucimlrepo

import numpy as np
import matplotlib.pyplot as plt

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from ucimlrepo import fetch_ucirepo

# fetch dataset
breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)

# data (as pandas dataframes)
X = breast_cancer_wisconsin_diagnostic.data.features
y = breast_cancer_wisconsin_diagnostic.data.targets
y = y.replace({'B': 0, 'M': 1})
y=np.ravel(y)

# Do matrix multiplication with numpy.dot
X = np.where(X == 0, 1e-10, X)  # Replace zero values with a small positive value
X = np.log(X)
# metadata
print(breast_cancer_wisconsin_diagnostic.metadata)

# variable information
print(breast_cancer_wisconsin_diagnostic.variables)

print(y)

"""###Step 2: Define the Sigmoid Function

The logistic regression model uses the sigmoid function to map predicted values to probabilities:

"""

def sigmoid(z):
  return 1 / (1 + np.exp(-z))

"""### Step 3: Define the Cost Function

For logistic regression, the cost function (often called the log loss)
"""

def computeCost(theta, X, y):
  m = len(X)
  global h_theta
  h_theta = sigmoid(np.dot(X, theta))
  term1 = np.dot(-y.T, np.log(h_theta))
  term2 = np.dot((1 - y).T, np.log(1 - h_theta))
  cost = np.sum(term1 - term2) / m

  # Ensure cost is an array
  if not isinstance(cost, np.ndarray):
    cost = np.array([cost])


  gradient = 1/m * np.dot(X.transpose(), (h_theta - y)) # 1/m (y^-y).x
  return cost[0], gradient

"""###Step 4: Gradient Descent

Here we will adjust the parameters iteratively to minimize the cost:
"""

def gradient_descent(X, y, theta, alpha, num_iterations):
  m = len(y)
  cost_history = []

  for i in range(num_iterations):
    cost, gradient = computeCost(theta, X, y)
    theta -= alpha * gradient
    cost_history.append(cost)
  return theta, cost_history

"""### Step 5: Training the Model

Initialize the parameters and run the gradient descent:
"""

# Add an intercept column to X
X = np.hstack((np.ones((X.shape[0], 1)), X))
#print(X)
theta = np.zeros(X.shape[1])
alpha = 0.01
num_iterations = 1000
theta, cost_history = gradient_descent(X, y, theta, alpha, num_iterations)
print(f"Estimated parameters: {theta}")
print(f"Estimated parameters: {cost_history}")

"""### Step 6: Plotting the Cost Function

To ensure that our gradient descent algorithm worked, we should see a decreasing cost over
iterations:
"""

plt.plot(cost_history)
plt.xlabel("Iteration")
plt.ylabel("$J(\Theta)$")
plt.title("Cost function using Gradient Descent")
plt.show()

"""This should show a plot where the cost decreases over the iterations, indicating that our gradient
descent algorithm is converging and optimizing the logistic regression parameters.

### Step 7: Evaluating Classifier
"""

y_pred = np.where(h_theta > 0.5, 1, 0)
# Calculate accuracy
accuracy = accuracy_score(y, y_pred)
print(f'Accuracy: {accuracy:.2f}')

# Generate a classification report
class_report = classification_report(y, y_pred)
print('Classification Report:\n', class_report)

# Create a confusion matrix
conf_matrix = confusion_matrix(y, y_pred)
print('Confusion Matrix:\n', conf_matrix)

from sklearn.metrics import confusion_matrix
import seaborn as sns
# Get the confusion matrix
cm = confusion_matrix(y, y_pred)
# Plotting the confusion matrix
plt.figure(figsize=(10,7))
sns.heatmap(cm, annot=True, fmt='g', cmap="Blues")
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""### **Conclusion:**

Due to the nature of the problem, selection of the appropriate classification model(high precision and accuracy) such as the Logistic Regression classifier, is necessary.


The model produces an accuracy of 94% further displayed in the confusion matrix. With such a high accuracy, the model can be used to accurately predict the classes of breast cancer: Benign or Malignant.
"""